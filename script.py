import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_score, recall_score, f1_score, make_scorer
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import LeaveOneOut
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('ClaMP_Integrated-5184.csv')

df.shape

df.columns

df.info()

df.describe()

df.isnull().sum()

df.duplicated().sum()

# remove duplicates
df.drop_duplicates(inplace=True)
df.duplicated().sum()

df['packer_type'].value_counts()

# store the unique packer types in a list
packer_types = list(df['packer_type'].unique())

# create a dictionary to store the packer types and their corresponding values
packer_type_dict = {}

# loop over the packer types list and assign each packer type a value
for i in range(len(packer_types)):
    packer_type_dict[packer_types[i]] = i

# print the dictionary
packer_type_dict

# map the packer type values to the packer type column
for key, value in packer_type_dict.items():
    # replace the packer type with the corresponding value
    df['packer_type'] = df['packer_type'].replace(key, value)

# print the first five rows of the dataframe
df.head()

X = df.drop(['class'], axis=1)
y = df['class']

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Visualize the correlation matrix as a heatmap
plt.figure(figsize=(35, 35))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show()

# Select features with high correlation with the target variable (example: top 2)
num_features_to_select = 35
selector = SelectKBest(score_func=f_classif, k=num_features_to_select)
X_new = selector.fit_transform(X, y)

feature_names = list(X.columns.values)

# Get the indices of the selected features
selected_feature_indices = np.where(selector.get_support())[0]

# Get the names of the selected features
selected_feature_names = [feature_names[i] for i in selected_feature_indices]

print(f"Selected Features: {selected_feature_names}")

# Create a new DataFrame with selected features and the class column
df_selected_features = df[selected_feature_names + ['class']]
df_selected_features.head()

x = df_selected_features.drop(['class'], axis=1)
y = df_selected_features['class']

# Standardize the features
scaler = StandardScaler()
x = scaler.fit_transform(x)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42)

svc = SVC()

print("Evaluation for Support Vector Classifier".center(75, '_'))

svc.fit(X_train, y_train)

svc_prediction = svc.predict(X_test)

svc_accuracy = accuracy_score(y_test, svc_prediction)
svc_precision = precision_score(y_test, svc_prediction, average="weighted")
svc_recall = recall_score(y_test, svc_prediction, average="weighted")
svc_f1_score = f1_score(y_test, svc_prediction, average="weighted")

print("Prediciton:    ", svc_prediction)
print('_' * 75)

print("Accuracy:" + "\t" + f"{(svc_accuracy * 100)}%")
print("Precision:" + "\t" + f"{(svc_precision * 100)}%")
print("Recall:" + "\t\t" f"{(svc_recall * 100)}%")
print("F1-Score:" + "\t" + f"{(svc_f1_score * 100)}%")
print('_' * 75)

# plot the confusion matrix
cm = confusion_matrix(y_test, svc_prediction)
plt.figure(figsize=(10, 10))
sns.heatmap(cm, annot=True, fmt=".0f", linewidths=0.5,
            square=True, cmap="Blues_r")
plt.ylabel("Actual Label")
plt.xlabel("Predicted Label")
plt.title("Support Vector Classifier Confusion Matrix")
plt.show()

print("Classification Report For Decision Tree Classifier - 'Labels Test' & 'Prediction':")
print('-' * 97)
print(classification_report(y_test, svc_prediction))

print("ROC-AUC-Score: ", roc_auc_score(y_test, svc_prediction))
print('-' * 97)

# plot the ROC-AUC curve
fpr, tpr, thresholds = roc_curve(y_test, svc_prediction)
plt.figure(figsize=(10, 10))
plt.plot(fpr, tpr, linewidth=2, label=None)
plt.plot([0, 1], [0, 1], 'k--')
plt.axis([-0.01, 1, 0, 1])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Support Vector Classifier ROC Curve')
plt.show()

print("Evaluation for Decision Tree Classifier".center(75, '_'))

decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)

decision_tree_prediction = decision_tree.predict(X_test)
decision_tree_accuracy = accuracy_score(y_test, decision_tree_prediction)
decision_tree_precision = precision_score(
    y_test, decision_tree_prediction, average="weighted")
decision_tree_recall = recall_score(
    y_test, decision_tree_prediction, average="weighted")
decision_tree_f1_score = f1_score(
    y_test, decision_tree_prediction, average="weighted")

print("Prediciton:    ", decision_tree_prediction)
print('_' * 75)

print("Accuracy:" + "\t" + f"{(decision_tree_accuracy * 100)}%")
print("Precision:" + "\t" + f"{(decision_tree_precision * 100)}%")
print("Recall:" + "\t\t" f"{(decision_tree_recall * 100)}%")
print("F1-Score:" + "\t" + f"{(decision_tree_f1_score * 100)}%")
print('_' * 75)

# plot the confusion matrix
cm = confusion_matrix(y_test, decision_tree_prediction)
plt.figure(figsize=(10, 10))
sns.heatmap(cm, annot=True, fmt=".0f", linewidths=0.5,
            square=True, cmap="Blues_r")
plt.ylabel("Actual Label")
plt.xlabel("Predicted Label")
plt.title("Decision Tree Classifier Confusion Matrix")
plt.show()

print("Classification Report For Decision Tree Classifier - 'Labels Test' & 'Prediction':")
print('-' * 97)
print(classification_report(y_test, decision_tree_prediction))

print("ROC-AUC-Score: ", roc_auc_score(y_test, decision_tree_prediction))
print('-' * 97)

# plot the ROC-AUC curve
fpr, tpr, thresholds = roc_curve(y_test, decision_tree_prediction)
plt.figure(figsize=(10, 10))
plt.plot(fpr, tpr, linewidth=2, label=None)
plt.plot([0, 1], [0, 1], 'k--')
plt.axis([-0.01, 1, 0, 1])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Support Vector Classifier ROC Curve')
plt.show()

# SVC classifier (svc)

# Define the scoring metrics you want to use
scoring_metrics = {
    'Accuracy': make_scorer(accuracy_score),
    'Precision': make_scorer(precision_score, average='weighted'),
    'Recall': make_scorer(recall_score, average='weighted'),
    'F1-Score': make_scorer(f1_score, average='weighted')
}

# Perform k-fold cross-validation (e.g., k=5)
k = 5
for metric, scoring_metric in scoring_metrics.items():
    scores = cross_val_score(svc, X, y, cv=k, scoring=scoring_metric)
    mean_score = scores.mean()
    print(f"{metric} (k={k}-fold): {mean_score}")

# store the scores
scv_k_fold_accuracy = scores[0]
scv_k_fold_precision = scores[1]
scv_k_fold_recall = scores[2]
scv_k_fold_f1_score = scores[3]

# Decision Tree classifier (decision_tree)

# Define the scoring metrics you want to use
scoring_metrics = {
    'Accuracy': make_scorer(accuracy_score),
    'Precision': make_scorer(precision_score, average='weighted'),
    'Recall': make_scorer(recall_score, average='weighted'),
    'F1-Score': make_scorer(f1_score, average='weighted')
}

# Perform k-fold cross-validation (e.g., k=5)
k = 5
for metric, scoring_metric in scoring_metrics.items():
    scores = cross_val_score(decision_tree, X, y, cv=k, scoring=scoring_metric)
    mean_score = scores.mean()
    print(f"{metric} (k={k}-fold): {mean_score}")

# store the scores
decision_tree_k_fold_accuracy = scores[0]
decision_tree_k_fold_precision = scores[1]
decision_tree_k_fold_recall = scores[2]
decision_tree_k_fold_f1_score = scores[3]

# Decision Tree classifier (decision_tree)

# Define the scoring metrics you want to use
scoring_metrics = {
    'Accuracy': make_scorer(accuracy_score),
    'Precision': make_scorer(precision_score, average='weighted'),
    'Recall': make_scorer(recall_score, average='weighted'),
    'F1-Score': make_scorer(f1_score, average='weighted')
}

cv = LeaveOneOut()

scores = cross_val_score(svc, X, y, cv=cv, scoring=scoring_metric)
mean_score = scores.mean()
print(f"{metric} (k={k}-fold): {mean_score}")

# store the scores
scv_loocv_accuracy = scores[0]
scv_loocv_precision = scores[1]
scv_loocv_recall = scores[2]
scv_loocv_f1_score = scores[3]

# Decision Tree classifier (decision_tree)

# Define the scoring metrics you want to use
scoring_metrics = {
    'Accuracy': make_scorer(accuracy_score),
    'Precision': make_scorer(precision_score, average='weighted'),
    'Recall': make_scorer(recall_score, average='weighted'),
    'F1-Score': make_scorer(f1_score, average='weighted')
}

cv = LeaveOneOut()

scores = cross_val_score(decision_tree, X, y, cv=cv, scoring=scoring_metric)
mean_score = scores.mean()
print(f"{metric} (k={k}-fold): {mean_score}")

# store the scores
decision_tree_loocv_accuracy = scores[0]
decision_tree_loocv_precision = scores[1]
decision_tree_loocv_recall = scores[2]
decision_tree_loocv_f1_score = scores[3]

svc.fit(X_train, y_train)

svc_prediction = svc.predict(X_test)

svc_accuracy = accuracy_score(y_test, svc_prediction)
svc_precision = precision_score(y_test, svc_prediction, average="weighted")
svc_recall = recall_score(y_test, svc_prediction, average="weighted")
svc_f1_score = f1_score(y_test, svc_prediction, average="weighted")

print("Prediciton:    ", svc_prediction)
print('_' * 75)

print("Accuracy:" + "\t" + f"{(svc_accuracy * 100)}%")
print("Precision:" + "\t" + f"{(svc_precision * 100)}%")
print("Recall:" + "\t\t" f"{(svc_recall * 100)}%")
print("F1-Score:" + "\t" + f"{(svc_f1_score * 100)}%")
print('_' * 75)

decision_tree.fit(X_train, y_train)

decision_tree_prediction = decision_tree.predict(X_test)
decision_tree_accuracy = accuracy_score(y_test, decision_tree_prediction)
decision_tree_precision = precision_score(
    y_test, decision_tree_prediction, average="weighted")
decision_tree_recall = recall_score(
    y_test, decision_tree_prediction, average="weighted")
decision_tree_f1_score = f1_score(
    y_test, decision_tree_prediction, average="weighted")

print("Prediciton:    ", decision_tree_prediction)
print('_' * 75)

print("Accuracy:" + "\t" + f"{(decision_tree_accuracy * 100)}%")
print("Precision:" + "\t" + f"{(decision_tree_precision * 100)}%")
print("Recall:" + "\t\t" f"{(decision_tree_recall * 100)}%")
print("F1-Score:" + "\t" + f"{(decision_tree_f1_score * 100)}%")
print('_' * 75)


print("Evaluation for Decision Tree Classifier".center(75, '_'))

decision_tree = DecisionTreeClassifier(
    max_depth=15, random_state=42, criterion='gini', splitter='best')
decision_tree.fit(X_train, y_train)

decision_tree_prediction = decision_tree.predict(X_test)

decision_tree_accuracy_ht = accuracy_score(y_test, decision_tree_prediction)
decision_tree_precision_ht = precision_score(
    y_test, decision_tree_prediction, average="weighted")
decision_tree_recall_ht = recall_score(
    y_test, decision_tree_prediction, average="weighted")
decision_tree_f1_score_ht = f1_score(
    y_test, decision_tree_prediction, average="weighted")

print("Prediciton:    ", decision_tree_prediction)
print('_' * 75)

print("Accuracy:" + "\t" + f"{(decision_tree_accuracy_ht * 100)}%")
print("Precision:" + "\t" + f"{(decision_tree_precision_ht * 100)}%")
print("Recall:" + "\t\t" f"{(decision_tree_recall_ht * 100)}%")
print("F1-Score:" + "\t" + f"{(decision_tree_f1_score_ht * 100)}%")
print('_' * 75)

svc = SVC(kernel='rbf', C=1, gamma=0.1, random_state=42)

print("Evaluation for Support Vector Classifier".center(75, '_'))

svc.fit(X_train, y_train)

svc_prediction = svc.predict(X_test)

svc_accuracy_ht = accuracy_score(y_test, svc_prediction)
svc_precision_ht = precision_score(y_test, svc_prediction, average="weighted")
svc_recall_ht = recall_score(y_test, svc_prediction, average="weighted")
svc_f1_score_ht = f1_score(y_test, svc_prediction, average="weighted")

print("Prediciton:    ", svc_prediction)
print('_' * 75)

print("Accuracy:" + "\t" + f"{(svc_accuracy_ht * 100)}%")
print("Precision:" + "\t" + f"{(svc_precision_ht * 100)}%")
print("Recall:" + "\t\t" f"{(svc_recall_ht * 100)}%")
print("F1-Score:" + "\t" + f"{(svc_f1_score_ht * 100)}%")
print('_' * 75)


# Your data
dt_acc_scores = [decision_tree_accuracy, decision_tree_precision,
                 decision_tree_recall, decision_tree_f1_score]
dt_acc_scores_ht = [decision_tree_accuracy_ht, decision_tree_precision_ht,
                    decision_tree_recall_ht, decision_tree_f1_score_ht]
svc_acc_scores = [svc_accuracy, svc_precision, svc_recall, svc_f1_score]
svc_acc_scores_ht = [svc_accuracy_ht,
                     svc_precision_ht, svc_recall_ht, svc_f1_score_ht]

x_grid = ["Accuracy Score", "Precision Score", "Recall Score", "F1-Score"]
y_grid = np.arange(0, 1.1, 0.1)  # Use np.arange for float values
xpos = np.arange(len(x_grid))
bar_width = 0.18

plt.rcParams["font.size"] = 8
plt.figure(figsize=(10, 7), dpi=700, facecolor="silver")
plt.title("Metric Visualization on all Regression Models")

# Create bar plots
bars = plt.bar(xpos, dt_acc_scores, width=bar_width,
               label="Decision Tree Classifier", color="green")
bars_ht = plt.bar(xpos + bar_width, dt_acc_scores_ht, width=bar_width,
                  label="Decision Tree Classifier (Hyperparameter Tuning)", color="orange")
bars_svc = plt.bar(xpos + 2 * bar_width, svc_acc_scores,
                   width=bar_width, label="Support Vector Classifier", color="blue")
bars_svc_ht = plt.bar(xpos + 3 * bar_width, svc_acc_scores_ht, width=bar_width,
                      label="Support Vector Classifier (Hyperparameter Tuning)", color="red")

# Annotate bars with percentage values
for bar, score in zip(bars, dt_acc_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
             f'{score:.2%}', ha='center', va='bottom')

for bar, score in zip(bars_ht, dt_acc_scores_ht):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
             f'{score:.2%}', ha='center', va='bottom')

for bar, score in zip(bars_svc, svc_acc_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
             f'{score:.2%}', ha='center', va='bottom')

for bar, score in zip(bars_svc_ht, svc_acc_scores_ht):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
             f'{score:.2%}', ha='center', va='bottom')

plt.xticks(xpos + 1.5 * bar_width, x_grid)
plt.yticks(y_grid)
plt.xlabel("Metrics")
plt.ylabel("Scores")
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()
